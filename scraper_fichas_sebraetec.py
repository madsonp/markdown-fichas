"""
Web Scraper para coletar PDFs das Fichas T√©cnicas Sebraetec
URL: https://datasebrae.com.br/fichas-tecnicas-sebraetec/
"""

import requests
from bs4 import BeautifulSoup
import time
from pathlib import Path
from urllib.parse import urljoin, urlparse
import re
from typing import List, Dict, Optional

try:
    from config import SEBRAETEC_BASE_URL, SCRAPER_USER_AGENT, SCRAPER_TIMEOUT, SCRAPER_DELAY, SCRAPER_MAX_RETRIES, ENTRADA_PDFS_DIR
    from logger_config import setup_logger, LogContext, log_exception
    logger = setup_logger(__name__)
    USE_NEW_INFRA = True
except ImportError:
    SEBRAETEC_BASE_URL = "https://datasebrae.com.br/fichas-tecnicas-sebraetec/"
    SCRAPER_USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    SCRAPER_TIMEOUT = 30
    SCRAPER_DELAY = 1.0
    SCRAPER_MAX_RETRIES = 3
    ENTRADA_PDFS_DIR = Path("entrada/pdfs")
    USE_NEW_INFRA = False


class SebraetecScraper:
    """Scraper para coletar PDFs das fichas t√©cnicas do Sebraetec"""
    
    def __init__(self, download_dir: Optional[Path] = None):
        self.base_url = SEBRAETEC_BASE_URL
        self.download_dir = download_dir or ENTRADA_PDFS_DIR
        self.download_dir.mkdir(parents=True, exist_ok=True)
        self.timeout = SCRAPER_TIMEOUT
        self.delay = SCRAPER_DELAY
        self.max_retries = SCRAPER_MAX_RETRIES
        
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': SCRAPER_USER_AGENT
        })
        
        if USE_NEW_INFRA:
            logger.info(f"Scraper inicializado - URL: {self.base_url}")
        
    def get_page_content(self, url: str, retries: int = 0) -> Optional[str]:
        """
        Obt√©m o conte√∫do HTML da p√°gina com retry logic
        
        Args:
            url: URL para acessar
            retries: N√∫mero de tentativas j√° realizadas
            
        Returns:
            Conte√∫do HTML ou None em caso de erro
        """
        try:
            response = self.session.get(url, timeout=self.timeout)
            response.raise_for_status()
            
            if USE_NEW_INFRA:
                logger.info(f"P√°gina carregada com sucesso: {url}")
            
            return response.text
            
        except requests.Timeout as e:
            if retries < self.max_retries:
                if USE_NEW_INFRA:
                    logger.warning(f"Timeout - Tentativa {retries + 1}/{self.max_retries}")
                time.sleep(self.delay * 2)
                return self.get_page_content(url, retries + 1)
            else:
                if USE_NEW_INFRA:
                    log_exception(logger, e, f"acessar {url} ap√≥s {self.max_retries} tentativas")
                else:
                    print(f"‚ùå Timeout ap√≥s {self.max_retries} tentativas: {url}")
                return None
                
        except requests.RequestException as e:
            if USE_NEW_INFRA:
                log_exception(logger, e, f"acessar {url}")
            else:
                print(f"‚ùå Erro ao acessar {url}: {e}")
            return None
    
    def extract_pdf_links(self, html_content: str) -> List[Dict[str, str]]:
        """
        Extrai todos os links de PDFs da p√°gina
        
        Args:
            html_content: Conte√∫do HTML da p√°gina
            
        Returns:
            Lista de dicion√°rios com url e texto dos links
        """
        if not html_content:
            return []
        
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            pdf_links = []
            
            # Buscar todos os links que apontam para PDFs
            for link in soup.find_all('a', href=True):
                href = link['href']
                if href.lower().en: str, filename: Optional[str] = None) -> Optional[Path]:
        """
        Faz download de um PDF com retry logic
        
        Args:
            pdf_url: URL do PDF
            filename: Nome do arquivo (opcional, ser√° extra√≠do da URL)
            
        Returns:
            Path do arquivo baixado ou None em caso de erro
        """
        try:
            if not filename:
                # Extrair nome do arquivo da URL
                parsed = urlparse(pdf_url)
                filename = Path(parsed.path).name
                
                # Limpar nome do arquivo
                filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
            
            filepath = self.download_dir / filename
            
            # Verificar se j√° existe
            if filepath.exists():
                if USE_NEW_INFRA:
                    logger.debug(f"Arquivo j√° existe: {filename}")
                else:
                    print(f"‚è≠Ô∏è  J√° existe: {filename}")
                return filepath
            
            if USE_NEW_INFRA:
                logger.info(f"Baixando: {filename}")
            else:
                print(f"‚¨áÔ∏è  Baixando: {filename}")
            
            response = self.session.get(pdf_url, stream=True, timeout=self.timeout * 2)
            response.raise_for_status()
            
            # Salvar arquivo
            with open(filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
            
            if USE_NEW_INFRA:
                logger.info(f"‚úÖ Salvo: {filepath.name}")
            else:
                print(f"‚úÖ Salvo: {filepath}")
            
            return filepath
            
        except requests.RequestException as e:
            if USE_NEW_INFRA:
                log_exception(logger, e, f"baixar {pdf_url}")
            else:
                print(f"‚ùå Erro ao baixar {pdf_url}: {e}")
            return None
        except IOError as e:
            if USE_NEW_INFRA:
                log_exception(logger, e, f"salvar arquivo {filename}")
            else:
                print(f"‚ùå Erro ao salvar {filename
            # Verificar se j√° existe
            if filepath.exists():
                print(f"‚è≠Ô∏è  J√° existe: {filename}")
                return filepath
            
            print(f"‚¨áÔ∏è  Baixando: {filename}")
            response = self.session.get(pdf_url, stream=True, timeout=60)
            response.raise_for_status()
            
            # Salvar arquivo
            with open(filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            print(f"‚úÖ Salvo: {filepath}")
            return filepath
            
        except requests.RequestException as e:
            print(f"‚ùå Erro ao baixar {pdf_url}: {e}")
            return None
    
    def scrape_all_pdfs(self) -> List[Path]:
        """
        Extrai e baixa todos os PDFs da p√°gina
        
        Returns:
            Lista de Paths dos arquivos baixados
        """
        print("="*60)
        print("üîç SCRAPER DE FICHAS T√âCNICAS SEBRAETEC")
        print("="*60)
        print(f"üìÅ Diret√≥rio de download: {self.download_dir.absolute()}")
        print(f"üåê URL: {self.base_url}")
        print()
        
        if USE_NEW_INFRA:
            with LogContext(logger, "Scraping de PDFs"):
                return self._scrape_implementation()
        else:
            return self._scrape_implementation()
    
    def _scrape_implementation(self) -> List[Path]:
        """Implementa√ß√£o interna do scraping"""
        # Obter conte√∫do da p√°gina
        print("üìÑ Carregando p√°gina principal...")
        html = self.get_page_content(self.base_url)
        
        if not html:
            print("‚ùå N√£o foi poss√≠vel carregar a p√°gina")
            return []
        
        # Extrair links de PDFs
        print("üîé Procurando PDFs na p√°gina...")
        pdf_links = self.extract_pdf_links(html)
        
        if not pdf_links:
            print("‚ö†Ô∏è  Nenhum link de PDF encontrado diretamente na p√°gina")
            print("üí° A p√°gina pode usar JavaScript para carregar os PDFs")
            print("üí° Vou procurar por links em elementos espec√≠ficos...")
            
            # Tentar abordagens alternativas
            soup = BeautifulSoup(html, 'html.parser')
            
            # Procurar por iframes, bot√µes de download, etc
            print("\nüîç Analisando estrutura da p√°gina...")
            print(f"   - Total de links encontrados: {len(soup.find_all('a'))}")
            print(f"   - Scripts JavaScript: {len(soup.find_all('script'))}")
            print(f"   - Iframes: {len(soup.find_all('iframe'))}")
            
            return []
        
        print(f"‚ú® Encontrados {len(pdf_links)} PDFs")
        print()
        
        # Baixar cada PDF
        downloaded = []
        for i, pdf_info in enumerate(pdf_links, 1):
            print(f"[{i}/{len(pdf_links)}] {pdf_info['text']}")
            filepath = self.download_pdf(pdf_info['url'])
            if filepath:
                downloaded.append(filepath)
            time.sleep(self.delay)  # Pausa entre downloads
        
        print()
        print("="*60)
        print(f"‚úÖ Download conclu√≠do: {len(downloaded)}/{len(pdf_links)} PDFs")
        print("="*60)
        
        if USE_NEW_INFRA:
            logger.info(f"Scraping conclu√≠do: {len(downloaded)} PDFs baixados")
        
        return downloaded


def main():
    """Fun√ß√£o principal do scraper"""
    scraper = SebraetecScraper()
    pdfs = scraper.scrape_all_pdfs()
    
    if pdfs:
        print("\nüìã PDFs baixados:")
        for pdf in pdfs:
            print(f"   - {pdf.name}")
    else:
        print("\n‚ö†Ô∏è  Nenhum PDF foi baixado")
        print("\nüí° Sugest√µes:")
        print("   1. A p√°gina pode usar JavaScript din√¢mico - considere usar Selenium")
        print("   2. Pode haver um seletor de estado/filtro que precisa ser acionado")
        print("   3. Verifique manualmente a p√°gina para identificar a estrutura")

if __name__ == "__main__":
    main()
