"""
Web Scraper para coletar PDFs das Fichas T√©cnicas Sebraetec
URL: https://datasebrae.com.br/fichas-tecnicas-sebraetec/
"""

import requests
from bs4 import BeautifulSoup
import os
import time
from pathlib import Path
from urllib.parse import urljoin, urlparse
import re

class SebraetecScraper:
    def __init__(self, download_dir="entrada/pdfs"):
        self.base_url = "https://datasebrae.com.br/fichas-tecnicas-sebraetec/"
        self.download_dir = Path(download_dir)
        self.download_dir.mkdir(parents=True, exist_ok=True)
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
        
    def get_page_content(self, url):
        """Obt√©m o conte√∫do HTML da p√°gina"""
        try:
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            return response.text
        except requests.RequestException as e:
            print(f"‚ùå Erro ao acessar {url}: {e}")
            return None
    
    def extract_pdf_links(self, html_content):
        """Extrai todos os links de PDFs da p√°gina"""
        if not html_content:
            return []
        
        soup = BeautifulSoup(html_content, 'html.parser')
        pdf_links = []
        
        # Buscar todos os links que apontam para PDFs
        for link in soup.find_all('a', href=True):
            href = link['href']
            if href.lower().endswith('.pdf'):
                full_url = urljoin(self.base_url, href)
                pdf_links.append({
                    'url': full_url,
                    'text': link.get_text(strip=True)
                })
        
        return pdf_links
    
    def download_pdf(self, pdf_url, filename=None):
        """Faz download de um PDF"""
        try:
            if not filename:
                # Extrair nome do arquivo da URL
                parsed = urlparse(pdf_url)
                filename = os.path.basename(parsed.path)
                
                # Limpar nome do arquivo
                filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
            
            filepath = self.download_dir / filename
            
            # Verificar se j√° existe
            if filepath.exists():
                print(f"‚è≠Ô∏è  J√° existe: {filename}")
                return filepath
            
            print(f"‚¨áÔ∏è  Baixando: {filename}")
            response = self.session.get(pdf_url, stream=True, timeout=60)
            response.raise_for_status()
            
            # Salvar arquivo
            with open(filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            print(f"‚úÖ Salvo: {filepath}")
            return filepath
            
        except requests.RequestException as e:
            print(f"‚ùå Erro ao baixar {pdf_url}: {e}")
            return None
    
    def scrape_all_pdfs(self):
        """Extrai e baixa todos os PDFs da p√°gina"""
        print("="*60)
        print("üîç SCRAPER DE FICHAS T√âCNICAS SEBRAETEC")
        print("="*60)
        print(f"üìÅ Diret√≥rio de download: {self.download_dir.absolute()}")
        print(f"üåê URL: {self.base_url}")
        print()
        
        # Obter conte√∫do da p√°gina
        print("üìÑ Carregando p√°gina principal...")
        html = self.get_page_content(self.base_url)
        
        if not html:
            print("‚ùå N√£o foi poss√≠vel carregar a p√°gina")
            return []
        
        # Extrair links de PDFs
        print("üîé Procurando PDFs na p√°gina...")
        pdf_links = self.extract_pdf_links(html)
        
        if not pdf_links:
            print("‚ö†Ô∏è  Nenhum link de PDF encontrado diretamente na p√°gina")
            print("üí° A p√°gina pode usar JavaScript para carregar os PDFs")
            print("üí° Vou procurar por links em elementos espec√≠ficos...")
            
            # Tentar abordagens alternativas
            soup = BeautifulSoup(html, 'html.parser')
            
            # Procurar por iframes, bot√µes de download, etc
            print("\nüîç Analisando estrutura da p√°gina...")
            print(f"   - Total de links encontrados: {len(soup.find_all('a'))}")
            print(f"   - Scripts JavaScript: {len(soup.find_all('script'))}")
            print(f"   - Iframes: {len(soup.find_all('iframe'))}")
            
            return []
        
        print(f"‚ú® Encontrados {len(pdf_links)} PDFs")
        print()
        
        # Baixar cada PDF
        downloaded = []
        for i, pdf_info in enumerate(pdf_links, 1):
            print(f"[{i}/{len(pdf_links)}] {pdf_info['text']}")
            filepath = self.download_pdf(pdf_info['url'])
            if filepath:
                downloaded.append(filepath)
            time.sleep(1)  # Pausa entre downloads
        
        print()
        print("="*60)
        print(f"‚úÖ Download conclu√≠do: {len(downloaded)}/{len(pdf_links)} PDFs")
        print("="*60)
        
        return downloaded

def main():
    scraper = SebraetecScraper()
    pdfs = scraper.scrape_all_pdfs()
    
    if pdfs:
        print("\nüìã PDFs baixados:")
        for pdf in pdfs:
            print(f"   - {pdf.name}")
    else:
        print("\n‚ö†Ô∏è  Nenhum PDF foi baixado")
        print("\nüí° Sugest√µes:")
        print("   1. A p√°gina pode usar JavaScript din√¢mico - considere usar Selenium")
        print("   2. Pode haver um seletor de estado/filtro que precisa ser acionado")
        print("   3. Verifique manualmente a p√°gina para identificar a estrutura")

if __name__ == "__main__":
    main()
